{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17.3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Recommendation System using Association Rules Evaluation\n",
    "\"\"\"\n",
    "\n",
    "#%pip uninstall mlxtend\n",
    "#%pip install mlxtend==0.17.3\n",
    "\n",
    "import mlxtend\n",
    "print(mlxtend.__version__)\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %pip install apyori\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import networkx as nx\n",
    "import warnings\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy.stats as stats\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from itertools import combinations\n",
    "from apyori import apriori\n",
    "import warnings\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "transaction_df = pd.read_csv(r\"C:\\Users\\User\\Downloads\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey CSV\\transaction_data.csv\")\n",
    "couponred_df = pd.read_csv(r\"C:\\Users\\User\\Downloads\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey CSV\\coupon_redempt.csv\")\n",
    "coupon_df = pd.read_csv(r\"C:\\Users\\User\\Downloads\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey CSV\\coupon.csv\")\n",
    "campaign_df = pd.read_csv(r\"C:\\Users\\User\\Downloads\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey CSV\\campaign_table.csv\")\n",
    "campaigndesc_df = pd.read_csv(r\"C:\\Users\\User\\Downloads\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey CSV\\campaign_desc.csv\")\n",
    "causal_df = pd.read_csv(r\"C:\\Users\\User\\Downloads\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey CSV\\causal_data.csv\")\n",
    "product_df = pd.read_csv(r\"C:\\Users\\User\\Downloads\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey CSV\\product.csv\")\n",
    "demographic_df = pd.read_csv(r\"C:\\Users\\User\\Downloads\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey\\dunnhumby_The-Complete-Journey CSV\\hh_demographic.csv\")\n",
    "\n",
    "campaigndesc_df['Duration'] = campaigndesc_df['END_DAY'] - campaigndesc_df['START_DAY']\n",
    "\n",
    "# We merge campaign desc and Campaign Table by Campaign\n",
    "campaign = pd.merge(\n",
    "    campaigndesc_df[['CAMPAIGN', 'DESCRIPTION', 'START_DAY', 'END_DAY', 'Duration']],\n",
    "    campaign_df[['household_key', 'CAMPAIGN']],\n",
    "    on=\"CAMPAIGN\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "campaign['total_campaign'] = campaign.groupby('household_key')['CAMPAIGN'].transform('count')\n",
    "campaign.drop_duplicates(subset=['household_key', 'total_campaign'], keep=\"first\", inplace=True)\n",
    "\n",
    "# Calculate redemptions per household\n",
    "redemptions_per_household = couponred_df.groupby('household_key')['CAMPAIGN'].count()\n",
    "\n",
    "campaignred = pd.merge(campaign, redemptions_per_household, on=['household_key'], how=\"left\")\n",
    "\n",
    "customertrans = pd.merge(transaction_df, demographic_df, on='household_key', how='left')\n",
    "customertrans.dropna(inplace=True)\n",
    "\n",
    "transacts = pd.merge(customertrans, product_df[['PRODUCT_ID', 'DEPARTMENT', 'COMMODITY_DESC']], on='PRODUCT_ID', how='left')\n",
    "final_df = pd.merge(transacts, campaignred, on='household_key', how='left')\n",
    "final_df = final_df.drop('CAMPAIGN_y', axis=1)\n",
    "final_df = final_df.rename(columns={'CAMPAIGN_x': 'CAMPAIGN'})\n",
    "\n",
    "# Drop irrelevant columns for market basket analysis\n",
    "columns_to_drop = ['household_key', 'DAY', 'QUANTITY', 'STORE_ID', 'RETAIL_DISC', 'TRANS_TIME', 'WEEK_NO',\n",
    "                   'classification_4', 'KID_CATEGORY_DESC', 'CAMPAIGN', 'START_DAY', 'END_DAY', 'Duration', 'total_campaign']\n",
    "final_df = final_df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "final_df = final_df.dropna(subset=['PRODUCT_ID', 'COMMODITY_DESC'])\n",
    "final_df['DEPARTMENT'] = final_df['DEPARTMENT'].fillna('Unknown')\n",
    "final_df['COMMODITY_DESC'] = final_df['COMMODITY_DESC'].fillna('Unknown')\n",
    "\n",
    "# Group transactions by 'BASKET_ID'\n",
    "grouped = final_df.groupby('BASKET_ID')['COMMODITY_DESC'].apply(list)\n",
    "transactions_df = grouped.reset_index()\n",
    "transactions_df.columns = ['BASKET_ID', 'ITEMS']\n",
    "\n",
    "# Use random sampling to keep a sample of 20000\n",
    "transactions_df = transactions_df.sample(n=20000, random_state=42)\n",
    "\n",
    "# Filter out less frequent items\n",
    "item_counts = transactions_df['ITEMS'].explode().value_counts()\n",
    "top_items = item_counts[item_counts > 10].index\n",
    "transactions_df['ITEMS'] = transactions_df['ITEMS'].apply(lambda x: [item for item in x if item in top_items])\n",
    "\n",
    "# Save the DataFrame to a pickle file\n",
    "import os\n",
    "if not os.path.exists('transactions_df.pkl'):\n",
    "    transactions_df.to_pickle('transactions_df.pkl')\n",
    "\n",
    "# Process transactions to generate association rules\n",
    "def process_transactions(transactions_df, min_support=0.005):\n",
    "    \"\"\"\n",
    "    Process transactions and generate association rules\n",
    "    \n",
    "    Args:\n",
    "        transactions_df (pd.DataFrame): DataFrame with transaction items\n",
    "        min_support (float): Minimum support threshold for frequent itemsets\n",
    "\n",
    "    Returns:\n",
    "        tuple: Processed DataFrame, frequent itemsets, and association rules\n",
    "    \"\"\"\n",
    "    # One-hot encode using MultiLabelBinarizer with sparse output\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    transactions_encoded_sparse = mlb.fit_transform(transactions_df['ITEMS'])\n",
    "\n",
    "    # Convert sparse matrix to DataFrame\n",
    "    columns = mlb.classes_\n",
    "    transactions_encoded = pd.DataFrame.sparse.from_spmatrix(\n",
    "        transactions_encoded_sparse, columns=columns\n",
    "    )\n",
    "\n",
    "    # Find frequent itemsets\n",
    "    frequent_itemsets = fpgrowth(transactions_encoded, min_support=min_support, use_colnames=True)\n",
    "\n",
    "    if len(frequent_itemsets) == 0:\n",
    "        print(\"No frequent itemsets found. Try lowering min_support.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Generate association rules\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "    filtered_rules = rules[(rules['support'] > 0.01) & (rules['lift'] > 3)]\n",
    "    top_20_rules = filtered_rules.nlargest(20, 'lift')\n",
    "\n",
    "    return transactions_encoded, rules, top_20_rules\n",
    "\n",
    "def recommend_products(purchased_items, rules, top_n=5):\n",
    "    \"\"\"Recommend products based on association rules\"\"\"\n",
    "    related_rules = rules[\n",
    "        rules['antecedents'].apply(lambda x: set(purchased_items).issubset(x))\n",
    "    ]\n",
    "\n",
    "    if related_rules.empty:\n",
    "        return []\n",
    "\n",
    "    ranked_rules = related_rules.sort_values(\n",
    "        by=['lift', 'confidence'],\n",
    "        ascending=[False, False]\n",
    "    )\n",
    "\n",
    "    recommended_items = set()\n",
    "    for _, row in ranked_rules.iterrows():\n",
    "        recommended_items.update(row['consequents'])\n",
    "        if len(recommended_items) >= top_n:\n",
    "            break\n",
    "\n",
    "    return list(recommended_items)[:top_n]\n",
    "\n",
    "def evaluate_recommender(test_df, rules, top_n=5):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendation system\n",
    "    \n",
    "    Args:\n",
    "        test_df (pd.DataFrame): DataFrame of testing transactions\n",
    "        rules (pd.DataFrame): Association rules DataFrame\n",
    "        top_n (int): Number of recommendations to return\n",
    "\n",
    "    Returns:\n",
    "        tuple: True and predicted labels\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for index, row in test_df.iterrows():\n",
    "        purchased_items = row['ITEMS']\n",
    "        recommended_items = recommend_products(purchased_items, rules, top_n=top_n)\n",
    "        \n",
    "        # True labels are the actual items purchased, excluding the input items\n",
    "        true_items = set(purchased_items) - set(recommended_items)\n",
    "        \n",
    "        # Predicted labels are the recommended items\n",
    "        y_true.append(list(true_items))\n",
    "        y_pred.append(recommended_items)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Split transactions into training and testing sets\n",
    "train_df, test_df = train_test_split(transactions_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generate rules using only the training data\n",
    "transactions_encoded, rules, _ = process_transactions(train_df, min_support=0.001)\n",
    "\n",
    "if transactions_encoded is not None and rules is not None:\n",
    "    # Get true and predicted labels\n",
    "    y_true, y_pred = evaluate_recommender(test_df, rules)\n",
    "\n",
    "    # Flatten the list of lists to compute metrics\n",
    "    y_true_flat = [item for sublist in y_true for item in sublist]\n",
    "    y_pred_flat = [item for sublist in y_pred for item in sublist]\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(y_true_flat, y_pred_flat, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true_flat, y_pred_flat, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true_flat, y_pred_flat, average='macro', zero_division=0)\n",
    "\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
